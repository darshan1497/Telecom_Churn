# -*- coding: utf-8 -*-
"""Telecom_Churn_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    http://localhost:8888/notebooks/OneDrive/Desktop/Final-Hackathon-march/Telecome-churn-dataset/Telecom_Churn_Prediction.ipynb
"""

# Importing all necessary Libaries

import numpy as np # numpy used for mathematical operation on array
import pandas as pd # pandas used for data manipulation on dataframe
import matplotlib.pyplot as plt # matplotlib used for data visualization
import seaborn as sns # seaborn used for data visualization
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Read the data by using pandas
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")





# Reading first 5 Rows of the train and test data 
train.head()

test.head()

# checking shape of the train and test data
print(train.shape)
print( )
print(test.shape)

# drop the customer_ID column which does not have an much impact on data and also dont have any statistical importance becuse it is an automiticaly generated number
train.drop(columns=['customerID'], axis=1, inplace=True)

print(train.shape)
print( )
print(test.shape) 

# Reading the name of the columns of the train and test data
print(train.columns)
print( )
print(test.columns)

# checking the data types of each columns of train dataset and test data set
train.dtypes

test.dtypes

# checking the descriptive statistics of numeric variables
train.describe() 

# checking the non-null values and geting the information of all columns of train and test data
train.info()

test.info()

# checking for the missing values in the train and test data
train.isnull().sum()

test.isnull().sum()
# checking for the unique values present in the train and test data
train.nunique()

# checking the distribution of the target column Churn
count = sns.countplot(x=train['Churn'], data=train)
for bar in count.patches:
    count.annotate(format(bar.get_height()),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=11, xytext=(0, 8),
                   textcoords='offset points')
plt.show()
train['Churn'].value_counts().plot(kind="pie", autopct='%1.1f%%')

# visualizing the counts of churners withrespect to each columns
# creatin a predictor 
for i, predictor in enumerate(train.drop(columns=['Churn', 'TotalCharges', 'MonthlyCharges'])):
    plt.figure(i)
    sns.countplot(data=train, x=predictor, hue='Churn')

train['TotalCharges'] = pd.to_numeric(train['TotalCharges'], errors='coerce')
train.dtypesprint(test.shape)

train.isnull().sum()  
train[train['TotalCharges'].isnull()==True]

# the total number of null values present are very less compared to the dataset. so we can fill them with median of the Totalchrges column
train['TotalCharges'].median() 

train['TotalCharges'].fillna(1397.475, inplace=True)

train.isnull().sum()

# visualizing the relationship between monthly charges and total charges
sns.lmplot(data=train, x='MonthlyCharges', y='TotalCharges')

test['TotalCharges'] = pd.to_numeric(test['TotalCharges'], errors='coerce')

test.dtypes

test.isnull().sum()  

# Converting the churn data types into numeric values that is assigning a values as Yes=1, No=0
train['Churn'] = np.where(train.Churn == 'Yes',1,0)

train.head()

train.dtypes

# seperating the categorical columnas and numerical columns

train_cat_colms = [column for column in train.columns if train[column].dtype=='object']
print(train_cat_colms)

print(' ')

train_num_colms = [column for column in train.columns if train[column].dtype!='object']
print(train_num_colms)

# converting categorical values to numerical values 
from sklearn.preprocessing import LabelEncoder
for i in train_cat_colms:
    le = LabelEncoder()
    train[i] = le.fit_transform(train[i])

train.head()

train.shape

test_cat_colms = [column for column in test.columns if test[column].dtype=='object']
print(test_cat_colms)

print(' ')

test_num_colms = [column for column in test.columns if test[column].dtype!='object']
print(test_num_colms)
  

for i in test_cat_colms:
    le = LabelEncoder()
    test[i] = le.fit_transform(test[i])
    
test.head()
test.shape

# checking the correlation between the features of the train dataframe by ploting heatmap correlation
train_corr = train.corr() # finding the correlation of the each columns in the train dataframe, the correlation matrix to be visualized
plt.figure(figsize=(12,6)) # specify the width and height of the plot
trainplot = sns.heatmap(train_corr, annot=True,) # create a heatmap plot of a correlation matrix called "train_corr", with annotations displayed on the plot. "annot=True": enables the display of annotations on the heatmap, showing the correlation values for each pair of variables.
plt.title('Correlation plot') # giving the title to the plot 
plt.show() # display the figure

train.describe()

# Visulaizing the Pairplot of complete dataset
sns.pairplot(train, hue = 'Churn')

X = train.drop('Churn', axis=True)
X.head()

X.shape

Y = train['Churn']
Y.head()

Y.shape


# visualizing importance of features using ExtraTreeClassifier 
from sklearn.ensemble import ExtraTreesClassifier
# Instantiate the model
model = ExtraTreesClassifier()

# Fit the model on training data
model.fit(X, Y)

print(model.feature_importances_)
# plot graph of feature importances for better visualization 
plt.figure(figsize = (8,6))
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()

from imblearn.over_sampling import SMOTE
# Create an instance of SMOTE
sm = SMOTE(random_state=42)
# Fit and apply SMOTE
X_resampled, y_resampled = sm.fit_resample(X, Y)

test.shape



# Separating the Training and testing Data

X_train, X_test, Y_train, Y_test = train_test_split(X_resampled, y_resampled,test_size=0.2,random_state=0)

print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

# RandomForest Classifier
# Train a Random Forest classifier with 100 trees
rf_clf = RandomForestClassifier()
rf_clf.fit(X_train, Y_train)
l# Predict the labels for the test data
rfc_y_pred = rf_clf.predict(X_test)
rfc_y_pred
# Calculate the accuracy score of the model
rfc_acc = accuracy_score(Y_test, rfc_y_pred)
rfc_acc
# Model evaluation metrics
confusion_matrix(Y_test, rfc_y_pred)
rfc_report = classification_report(Y_test, rfc_y_pred)
print(rfc_report)

sns.distplot(Y_test-rfc_y_pred)
plt.show()

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]

# Create the random grid

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

# Random search of parameters, using 5 fold cross validation, 
# search across 100 different combinations
rf_random = RandomizedSearchCV(estimator = rf_clf, param_distributions = random_grid,scoring='accuracy',
                               n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1) 

rf_random.fit(X_train,Y_train)

rf_random.best_params_

prediction = rf_random.predict(X_test)
prediction

# Calculate the accuracy score of the model
rf_acc = accuracy_score(Y_test, prediction)
rf_acc

rf_report = classification_report(Y_test, prediction)
print(rf_report)

sns.distplot(Y_test-y_pred)
plt.show()



y_test_pred=rf_clf.predict(test)
y_test_pred

# submission file contains only one column that is Churn column which having the predicted column
submission_df = pd.DataFrame(y_test_pred, columns=['Churn'])

submission_df.shape

# converting the submission_df dataframe into csv file format
submission_df.to_csv('DarshanHM_submission.csv', index=False)

# Train and evaluate different models
models = [
    ('Logistic Regression', LogisticRegression()),
    ('Naive Bayes', GaussianNB()),
    ('SVM', svm.SVC()),
    ('KNN', KNeighborsClassifier()),
    ('Decision Tree', DecisionTreeClassifier()),
    ('Random Forest', RandomForestClassifier()),
]

for name, model in models:
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(Y_test, y_pred)
    print(name, "accuracy:", accuracy)
    
# Logistic regression and Randomforest models performs well on the dataset
# The Randomforest classifier model is selected for the further process that is for deployment 
# importing the pickle model
import pickle
# converting to pickle file 
pickle.dump(grid_search, open('rf_clf.pkl','wb'))
# rf_clf.pkl is file name which is created by open function and with write byte mode. 
# load the pickle file 
pickled_model=pickle.load(open('rf_clf.pkl','rb')) # rb = read byte mode
# prediction 
pickled_model.predict(test)



